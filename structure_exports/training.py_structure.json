{
  "project_info": {
    "name": "training.py",
    "path": "/Users/espensele/Desktop/Master/MasterOppg/Homomorphic-Neural-Cryptography/training.py",
    "generated_at": "2025-01-23T16:33:10.020922",
    "total_files": 1
  },
  "structure": {
     "training.py": {
      "type": "file",
      "info": {
        "mime_type": "text/x-python",
        "extension": ".py"
      }
  },
  "files": [
     {"path": "training.py",
      "content": "import os\n\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"4\"\nos.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n\nfrom tensorflow.keras.models import Model\nfrom numpy.typing import NDArray\nfrom typing import Callable, Tuple\nfrom data_utils.dataset_generator import generate_static_dataset, generate_cipher_dataset\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom neural_network.networks import create_networks\nfrom key.EllipticCurve import generate_key_pair, set_curve, get_key_shape\nfrom argparse import ArgumentParser\nimport sys\nimport random\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\n\nclass Training:\n    def __init__(self, batch_size: int, p1_bits: int, p2_bits: int, c3_bits: int, nonce_bits: int, curve: str, alice: Model, bob: Model, rate: float):\n        \"\"\"Initializes the Training class for training the HO models, Alice, Bob and Eve.\n        \n        Args:\n            batch_size: Number of samples in the dataset.\n            p1_bits: Number of bits in plaintext 1.\n            p2_bits: Number of bits in plaintext 2.\n            c3_bits: Number of bits in the output ciphertext.\n            nonce_bits: Number of bits in the nonce.\n            curve: Name of the elliptic curve.\n            alice: Alice Model.\n            bob: Bob Model.\n            rate: Dropout rate.\n        \"\"\"\n        self.p1_bits = p1_bits\n        self.p2_bits = p2_bits\n        self.c3_bits = c3_bits\n        self.nonce_bits = nonce_bits\n        self.curve = curve\n        self.alice = alice\n        self.bob = bob\n        self.batch_size = batch_size\n        self.abelosses, self.boblosses, self.evelosses = [], [], []\n        self.test_type = f\"ma-rate-{rate}-curve-{self.curve.name}-2\"\n        self.path = f'weights/weights-{self.test_type}'\n        isExist = os.path.exists(self.path)\n        if not isExist:\n            os.makedirs(self.path)\n\n    def train_HO_model(self, HO_model: Model, task: Callable[[NDArray[np.float64], NDArray[np.float64]], NDArray[np.float64]], op: NDArray[np.object_], filename: str):\n        \"\"\"Train the HO model.\n\n        Args:\n            HO_model: HO Model.\n            task: Task function which accepts 2 numpy arrays as arguments and returns a single numpy array as the result.\n            operation: Operation tag, numpy array of numpy arrays containing float64 elements.\n            filename: HO_model filename to save the weights.\n        \"\"\"\n        HO_model.trainable = True\n\n        # Train HO model on operation\n        X1_train, X2_train, y_train = generate_static_dataset(task, self.c3_bits, self.batch_size, seed=0)\n        X1_test, X2_test, y_test = generate_static_dataset(task, self.c3_bits, self.batch_size, mode=\"extrapolation\", seed=0)\n\n        HO_model.fit([op, X1_train, X2_train], y_train, batch_size=128, epochs=512,\n        verbose=2, validation_data=([op, X1_test, X2_test], y_test))\n\n        checkpoint = ModelCheckpoint(f'{self.path}/{filename}', monitor='val_loss',\n                                verbose=1, save_weights_only=True, save_best_only=True)\n        callbacks = [checkpoint]\n\n        # Train HO model with Alice to do addition on encrypted data\n        _, public_arr = generate_key_pair(self.batch_size, self.curve)\n        X1_cipher_train, X2_cipher_train, y_cipher_train = generate_cipher_dataset(self.p1_bits, self.p2_bits, self.batch_size, public_arr, self.alice, task, self.nonce_bits, 0)\n        X1_cipher_test, X2_cipher_test, y_cipher_test = generate_cipher_dataset(self.p1_bits, self.p2_bits, self.batch_size, public_arr, self.alice, task, self.nonce_bits, 1)\n\n        HO_model.fit([op, X1_cipher_train, X2_cipher_train], y_cipher_train, batch_size=128, epochs=512,\n            verbose=2, callbacks=callbacks, validation_data=([op, X1_cipher_test, X2_cipher_test], y_cipher_test))\n\n        # Save weights\n        HO_model.trainable = False\n    \n    def generate_batches(self) -> Tuple[NDArray[np.object_], NDArray[np.object_], NDArray[np.object_], NDArray[np.object_], NDArray[np.object_], NDArray[np.object_], NDArray[np.object_]]:\n        \"\"\"Generate batches of data for training.\n\n        Returns:\n            p1_batch, p2_batch, private_arr, public_arr, nonce, operation_a and operation_m.\n            p1_batch and p2_batch are numpy arrays of numpy arrays containing int64 elements, \n            while private_arr, public_arr, nonce, operation_a and operation_m are numpy arrays of numpy arrays containing float64 elements.\n        \"\"\"\n        p1_batch = np.random.randint(\n            0, 2, self.p1_bits * self.batch_size).reshape(self.batch_size, self.p1_bits)\n        p2_batch = np.random.randint(\n            0, 2, self.p2_bits * self.batch_size).reshape(self.batch_size, self.p2_bits)\n\n        private_arr, public_arr = generate_key_pair(self.batch_size, self.curve)\n\n        nonce = np.random.rand(self.batch_size, self.nonce_bits)\n\n        operation_a = np.zeros((self.batch_size, self.c3_bits))\n        operation_m = np.ones((self.batch_size, self.c3_bits))\n\n        return p1_batch, p2_batch, private_arr, public_arr, nonce, operation_a, operation_m\n    \n    def calculate_bob_loss(self, m_enc: NDArray[np.object_], private_arr: NDArray[np.object_], nonce: NDArray[np.object_], expected_output: NDArray[np.object_]) -> np.float64:\n        \"\"\"Calculate the loss for Bob's decryption.\n\n        Args:\n            m_enc: Encrypted message.\n            private_arr: Private key.\n            nonce: Nonce.\n            expected_output: Expected output.\n            m_enc, private_arr, nonce and expected_output are numpy arrays of numpy arrays containing float64 elements.\n        \n        Returns:\n            The mean of the sum of the absolute differences between the expected output and the decrypted message.\n        \"\"\"\n        m_dec = self.bob.predict([m_enc, private_arr, nonce])\n        return np.mean(np.sum(np.abs(expected_output - m_dec), axis=-1))\n    \n    def train(self, HO_model_addition: Model, HO_model_multiplication: Model, eve: Model, abhemodel: Model, evemodel: Model, n_epochs: int, m_train: int):\n        \"\"\"Train on encryption, decryption and eavesdropping.\n        \n        Args:\n            HO_model_addition: HO Addition Model.\n            HO_model_multiplication: HO Multiplication Model.\n            eve: Eve Model.\n            abhemodel: ABHE Model.\n            evemodel: Eve Model.\n            n_epochs: Number of epochs.\n            m_train: Size of the message space.\n        \"\"\"\n        alice_weights_path = f'{self.path}/alice_weights.h5'\n        bob_weights_path = f'{self.path}/bob_weights.h5'\n        eve_weights_path = f'{self.path}/eve_weights.h5'\n        n_batches = m_train // self.batch_size # iterations per epoch, training examples divided by batch size\n        epoch = 0\n        best_abeloss = float('inf')\n        best_epoch = 0\n        patience_epochs = 5\n        while epoch < n_epochs:\n            evelosses0 = []\n            boblosses0 = []\n            abelosses0 = []\n            for iteration in range(n_batches):\n\n                # Train the A-B+E network, train both Alice and Bob\n                self.alice.trainable = True\n                p1_batch, p2_batch, private_arr, public_arr, nonce, operation_a, operation_m = self.generate_batches()\n                loss = abhemodel.train_on_batch(\n                    [public_arr, p1_batch, p2_batch, nonce, private_arr, operation_a, operation_m], None)  # calculate the loss\n                    \n                # How well Alice's encryption and Bob's decryption work together\n                abelosses0.append(loss)\n                self.abelosses.append(loss)\n                abeavg = np.mean(abelosses0)\n\n                # Evaluate Bob's ability to decrypt a message\n                m1_enc, m2_enc = self.alice.predict([public_arr, p1_batch, p2_batch, nonce])\n                m3_enc_a = HO_model_addition.predict([operation_a, m1_enc, m2_enc])\n                m3_enc_m = HO_model_multiplication.predict([operation_m, m1_enc, m2_enc])\n\n                loss_m3_a = self.calculate_bob_loss(m3_enc_a, private_arr, nonce, p1_batch + p2_batch)\n                loss_m3_m = self.calculate_bob_loss(m3_enc_m, private_arr, nonce, p1_batch * p2_batch)\n                loss_m1 = self.calculate_bob_loss(m1_enc, private_arr, nonce, p1_batch)\n                loss_m2 = self.calculate_bob_loss(m2_enc, private_arr, nonce, p2_batch)\n                loss = (loss_m3_a + loss_m3_m + loss_m1 + loss_m2) / 4\n\n                boblosses0.append(loss)\n                self.boblosses.append(loss)\n                bobavg = np.mean(boblosses0)\n\n                # Train the EVE network\n                self.alice.trainable = False\n                p1_batch, p2_batch, _, public_arr, nonce, operation_a, operation_m = self.generate_batches()\n                loss = evemodel.train_on_batch([public_arr, p1_batch, p2_batch, nonce, operation_a, operation_m], None)\n\n                evelosses0.append(loss)\n                self.evelosses.append(loss)\n                eveavg = np.mean(evelosses0)\n\n                # Print progress\n                if iteration % max(1, (n_batches // 100)) == 0:\n                    print(\"\\rEpoch {:3}: {:3}% | abe: {:2.3f} | eve: {:2.3f} | bob: {:2.3f}\".format(\n                        epoch, 100 * iteration // n_batches, abeavg, eveavg, bobavg), end=\"\")\n                    sys.stdout.flush()\n\n            # Save weights for each improvement in Bob's loss\n            epoch_abeloss = np.mean(boblosses0)\n            if epoch_abeloss < best_abeloss:\n                best_abeloss = epoch_abeloss\n                best_epoch = epoch\n                self.alice.save_weights(alice_weights_path)\n                self.bob.save_weights(bob_weights_path)\n                eve.save_weights(eve_weights_path)\n                print(f\"\\nNew best Bob loss {best_abeloss} at epoch {epoch}\")\n            \n            # Early stopping\n            if epoch - best_epoch > patience_epochs:\n                print(f\"\\nEarly stopping: No improvement after {patience_epochs} epochs since epoch {best_epoch}. Best Bob loss: {best_abeloss}\")\n                break\n\n            epoch += 1\n\n        if not os.path.exists(alice_weights_path):\n            self.alice.save_weights(alice_weights_path)\n            self.bob.save_weights(bob_weights_path)\n            eve.save_weights(eve_weights_path)\n\n        print(\"Training complete.\")\n\n    def save_loss_values(self):\n        \"\"\"Save the loss values to a CSV file.\"\"\"\n        steps = -1\n        Biodata = {'ABloss': self.abelosses[:steps],\n                'Bobloss': self.boblosses[:steps],\n                'Eveloss': self.evelosses[:steps]}\n\n        df = pd.DataFrame(Biodata)\n\n        df.to_csv(f'dataset/{self.test_type}.csv', mode='a', index=False)\n\n\nif __name__ == \"__main__\":\n    # Set the seed for TensorFlow and any other random operation\n    seed = 0\n    tf.compat.v1.set_random_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n\n    parser = ArgumentParser()\n    parser.add_argument('-rate', type=float, default=0.1, help='Dropout rate')\n    parser.add_argument('-epoch', type=int, default=100, help='Number of epochs')\n    parser.add_argument('-batch', type=int, default=448, help='Batch size')\n    parser.add_argument('-curve', type=str, default=\"secp224r1\", help='Elliptic curve name')\n    args = parser.parse_args()\n\n    curve = set_curve(args.curve)\n\n    public_bits = get_key_shape(curve)[1]  \n    private_bits = get_key_shape(curve)[0]\n    dropout_rate = args.rate\n\n    alice, bob, HO_model_addition, eve, abhemodel, m_train, p1_bits, evemodel, p2_bits, learning_rate, c3_bits, nonce_bits, HO_model_multiplication = create_networks(public_bits, private_bits, dropout_rate)\n\n    training = Training(args.batch, p1_bits, p2_bits, c3_bits, nonce_bits, curve, alice, bob, args.rate)\n    training.train_HO_model(HO_model_addition, lambda x, y: x + y, np.zeros((args.batch, c3_bits)), \"addition_weights.h5\")\n    training.train_HO_model(HO_model_multiplication, lambda x, y: x * y, np.ones((args.batch, c3_bits)), \"multiplication_weights.h5\")\n    training.train(HO_model_addition, HO_model_multiplication, eve, abhemodel, evemodel, args.epoch, m_train)\n    training.save_loss_values()\n\n\n",
      "info": {
        "mime_type": "text/x-python",
        "extension": ".py"
        }}
  ]
}