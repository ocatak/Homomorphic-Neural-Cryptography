{
  "project_info": {
    "name": "neural_network",
    "path": "/Users/espensele/PycharmProjects/Homomorphic-Neural-Cryptography/neural_network",
    "generated_at": "2025-01-17T14:10:01.53488",
    "total_files": 4,
    "total_size": 21765
  },
  "structure": {
    "nac.py": {
      "type": "file",
      "info": {
        "size": 3913,
        "last_modified": "2025-01-17T14:10:01.512451",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    },
    "nalu.py": {
      "type": "file",
      "info": {
        "size": 6430,
        "last_modified": "2025-01-17T14:10:01.528693",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    },
    "__init__.py": {
      "type": "file",
      "info": {
        "size": 0,
        "last_modified": "2025-01-17T14:10:01.53181",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    },
    "networks.py": {
      "type": "file",
      "info": {
        "size": 11422,
        "last_modified": "2025-01-17T14:10:01.531978",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    }
  },
  "files": [
    {
      "path": "nac.py",
      "content": "from tensorflow.keras.layers import Layer\nfrom tensorflow.keras.layers import InputSpec\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import backend as K\n\nfrom tensorflow.keras.utils import get_custom_objects\n\n\nclass NAC(Layer):\n    def __init__(self, units,\n                 kernel_W_initializer='glorot_uniform',\n                 kernel_M_initializer='glorot_uniform',\n                 kernel_W_regularizer=None,\n                 kernel_M_regularizer=None,\n                 kernel_W_constraint=None,\n                 kernel_M_constraint=None,\n                 **kwargs):\n        \"\"\"\n        Neural Accumulator.\n\n        # Arguments:\n            units: Output dimension.\n            kernel_W_initializer: Initializer for `W` weights.\n            kernel_M_initializer: Initializer for `M` weights.\n            kernel_W_regularizer: Regularizer for `W` weights.\n            kernel_M_regularizer: Regularizer for `M` weights.\n            kernel_W_constraint: Constraints on `W` weights.\n            kernel_M_constraint: Constraints on `M` weights.\n            epsilon: Small factor to prevent log 0.\n\n        # Reference:\n        - [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)\n\n        \"\"\"\n        super(NAC, self).__init__()\n        self.units = units\n\n        self.kernel_W_initializer = initializers.get(kernel_W_initializer)\n        self.kernel_M_initializer = initializers.get(kernel_M_initializer)\n        self.kernel_W_regularizer = regularizers.get(kernel_W_regularizer)\n        self.kernel_M_regularizer = regularizers.get(kernel_M_regularizer)\n        self.kernel_W_constraint = constraints.get(kernel_W_constraint)\n        self.kernel_M_constraint = constraints.get(kernel_M_constraint)\n\n        self.supports_masking = True\n\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        self.W_hat = self.add_weight(shape=(input_dim, self.units),\n                                     name='W_hat',\n                                     initializer=self.kernel_W_initializer,\n                                     regularizer=self.kernel_W_regularizer,\n                                     constraint=self.kernel_W_constraint)\n\n        self.M_hat = self.add_weight(shape=(input_dim, self.units),\n                                     name='M_hat',\n                                     initializer=self.kernel_M_initializer,\n                                     regularizer=self.kernel_M_regularizer,\n                                     constraint=self.kernel_M_constraint)\n\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n\n    def call(self, inputs, **kwargs):\n        W = K.tanh(self.W_hat) * K.sigmoid(self.M_hat)\n        a = K.dot(inputs, W)\n        return a\n\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        assert input_shape[-1]\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)\n\n    def get_config(self):\n        config = {\n            'units': self.units,\n            'kernel_W_initializer': initializers.serialize(self.kernel_W_initializer),\n            'kernel_M_initializer': initializers.serialize(self.kernel_M_initializer),\n            'kernel_W_regularizer': regularizers.serialize(self.kernel_W_regularizer),\n            'kernel_M_regularizer': regularizers.serialize(self.kernel_M_regularizer),\n            'kernel_W_constraint': constraints.serialize(self.kernel_W_constraint),\n            'kernel_M_constraint': constraints.serialize(self.kernel_M_constraint),\n        }\n\n        base_config = super(NAC, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n\nget_custom_objects().update({'NAC': NAC})\n",
      "info": {
        "size": 3913,
        "last_modified": "2025-01-17T14:10:01.512451",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    },
    {
      "path": "nalu.py",
      "content": "from  tensorflow.keras.layers import Layer\nfrom  tensorflow.keras.layers import InputSpec\nfrom tensorflow.keras import initializers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import constraints\nfrom tensorflow.keras import backend as K\n\nfrom tensorflow.keras.utils import get_custom_objects\n\n# Build a custom Keras layer \nclass NALU(Layer):\n    def __init__(self, units,\n                 use_gating=True,\n                 kernel_W_initializer='glorot_uniform',\n                 kernel_M_initializer='glorot_uniform',\n                 gate_initializer='glorot_uniform',\n                 kernel_W_regularizer=None,\n                 kernel_M_regularizer=None,\n                 gate_regularizer=None,\n                 kernel_W_constraint=None,\n                 kernel_M_constraint=None,\n                 gate_constraint=None,\n                 epsilon=1e-7,\n                 **kwargs):\n        \"\"\"\n        Neural Arithmatic and Logical Unit.\n\n        # Arguments:\n            units: Output dimension.\n            use_gating: Bool, determines whether to use the gating\n                mechanism between W and m.\n            kernel_W_initializer: Initializer for `W` weights.\n            kernel_M_initializer: Initializer for `M` weights.\n            gate_initializer: Initializer for gate `G` weights.\n            kernel_W_regularizer: Regularizer for `W` weights.\n            kernel_M_regularizer: Regularizer for `M` weights.\n            gate_regularizer: Regularizer for gate `G` weights.\n            kernel_W_constraint: Constraints on `W` weights.\n            kernel_M_constraint: Constraints on `M` weights.\n            gate_constraint: Constraints on gate `G` weights.\n            epsilon: Small factor to prevent log 0.\n\n        # Reference:\n        - [Neural Arithmetic Logic Units](https://arxiv.org/abs/1808.00508)\n\n        \"\"\"\n        super(NALU, self).__init__()\n        self.units = units\n        self.use_gating = use_gating\n        self.epsilon = epsilon\n\n        self.kernel_W_initializer = initializers.get(kernel_W_initializer)\n        self.kernel_M_initializer = initializers.get(kernel_M_initializer)\n        self.gate_initializer = initializers.get(gate_initializer)\n        self.kernel_W_regularizer = regularizers.get(kernel_W_regularizer)\n        self.kernel_M_regularizer = regularizers.get(kernel_M_regularizer)\n        self.gate_regularizer = regularizers.get(gate_regularizer)\n        self.kernel_W_constraint = constraints.get(kernel_W_constraint)\n        self.kernel_M_constraint = constraints.get(kernel_M_constraint)\n        self.gate_constraint = constraints.get(gate_constraint)\n\n        self.supports_masking = True\n\n    # Method to create the weights of the layer.\n    # Defines weights W_hat, M_hat, and optionally G (if gating is used). \n    # The W_hat and M_hat weights are used for the arithmetic operations, while G is for gating between addition and multiplication.\n    def build(self, input_shape):\n        assert len(input_shape) >= 2\n        input_dim = input_shape[-1]\n\n        self.W_hat = self.add_weight(shape=(input_dim, self.units),\n                                     name='W_hat',\n                                     initializer=self.kernel_W_initializer,\n                                     regularizer=self.kernel_W_regularizer,\n                                     constraint=self.kernel_W_constraint)\n\n        self.M_hat = self.add_weight(shape=(input_dim, self.units),\n                                     name='M_hat',\n                                     initializer=self.kernel_M_initializer,\n                                     regularizer=self.kernel_M_regularizer,\n                                     constraint=self.kernel_M_constraint)\n\n        if self.use_gating:\n            self.G = self.add_weight(shape=(input_dim, self.units),\n                                     name='G',\n                                     initializer=self.gate_initializer,\n                                     regularizer=self.gate_regularizer,\n                                     constraint=self.gate_constraint)\n        else:\n            self.G = None\n\n        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n        self.built = True\n\n    # Method for computation\n    # m for multiplication (and division) and a for addition (and subtraction)\n    # If gating is used, it combines these transformations based on the gate's output.\n    def call(self, inputs, **kwargs):\n        W = K.tanh(self.W_hat) * K.sigmoid(self.M_hat)\n        m = K.exp(K.dot(K.log(K.abs(inputs) + self.epsilon), W))\n        a = K.dot(inputs, W)\n\n        if self.use_gating:\n            g = K.sigmoid(K.dot(inputs, self.G))\n            outputs = g * a + (1. - g) * m\n        else:\n            outputs = a + m\n\n        return outputs\n\n    # Method for computing the output shape of the layer.\n    def compute_output_shape(self, input_shape):\n        assert input_shape and len(input_shape) >= 2\n        assert input_shape[-1]\n        output_shape = list(input_shape)\n        output_shape[-1] = self.units\n        return tuple(output_shape)\n\n    # This method supports saving and loading of the model by returning the layer's configuration as a Python dictionary.\n    def get_config(self):\n        config = {\n            'units': self.units,\n            'use_gating': self.use_gating,\n            'kernel_W_initializer': initializers.serialize(self.kernel_W_initializer),\n            'kernel_M_initializer': initializers.serialize(self.kernel_M_initializer),\n            'gate_initializer': initializers.serialize(self.gate_initializer),\n            'kernel_W_regularizer': regularizers.serialize(self.kernel_W_regularizer),\n            'kernel_M_regularizer': regularizers.serialize(self.kernel_M_regularizer),\n            'gate_regularizer': regularizers.serialize(self.gate_regularizer),\n            'kernel_W_constraint': constraints.serialize(self.kernel_W_constraint),\n            'kernel_M_constraint': constraints.serialize(self.kernel_M_constraint),\n            'gate_constraint': constraints.serialize(self.gate_constraint),\n            'epsilon': self.epsilon\n        }\n\n        base_config = super(NALU, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))\n\n# Adding the NALU class to the set of available custom objects in Keras, allowing it to be used in model definitions by name.\nget_custom_objects().update({'NALU': NALU})\n",
      "info": {
        "size": 6430,
        "last_modified": "2025-01-17T14:10:01.528693",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    },
    {
      "path": "__init__.py",
      "content": "",
      "info": {
        "size": 0,
        "last_modified": "2025-01-17T14:10:01.53181",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    },
    {
      "path": "networks.py",
      "content": "from tensorflow.keras import backend as K\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Reshape, Flatten, Input, Dense, Conv1D, concatenate, Lambda, Dropout, Activation\nfrom tensorflow.keras.optimizers import RMSprop, Adam\nfrom key.EllipticCurve import get_key_shape, set_curve\nfrom neural_network.nalu import NALU\nfrom neural_network.nac import NAC\nfrom typing import Tuple\nfrom tensorflow import Tensor\n\n# Process plaintexts\ndef process_plaintext(ainput0: Tensor, ainput1: Tensor, anonce_input: Tensor, p_bits: int, public_bits: int, nonce_bits: int, dropout_rate: float, pad: str) -> Tensor:\n    \"\"\"Alice network process a plaintext.\n\n    Args:\n        ainput0: Public key input.\n        ainput1: Plaintext input.\n        anonce_input: Nonce input.\n        p_bits: Number of bits in the plaintext.\n        public_bits: Number of bits in the public key.\n        nonce_bits: Number of bits in the nonce.\n        dropout_rate: Dropout rate.\n        pad: Padding type.\n    \n    Returns:\n        The output of the Alice network.\n    \"\"\"\n    ainput = concatenate([ainput0, ainput1, anonce_input], axis=1)\n\n    adense1 = Dense(units=(p_bits + public_bits + nonce_bits), activation='tanh')(ainput)\n\n    dropout = Dropout(dropout_rate)(adense1, training=True)\n\n    areshape = Reshape((p_bits + public_bits + nonce_bits, 1,))(dropout)\n\n    aconv1 = Conv1D(filters=2, kernel_size=4, strides=1,\n                    padding=pad, activation='tanh')(areshape)\n\n    aconv2 = Conv1D(filters=4, kernel_size=2, strides=2,\n                    padding=pad, activation='tanh')(aconv1)\n\n    aconv3 = Conv1D(filters=4, kernel_size=1, strides=1,\n                    padding=pad, activation='tanh')(aconv2)\n\n    aconv4 = Conv1D(filters=1, kernel_size=1, strides=1,\n                    padding=pad, activation='hard_sigmoid')(aconv3)\n\n    return Flatten()(aconv4)\n\n# Alice network\ndef create_networks(public_bits: int, private_bits: int, dropout_rate: float\n) -> Tuple[Model, Model, Model, Model, Model, int, int, Model, int, float, int, int, Model]:\n    \"\"\"Creates the Alice, Bob, HO and Eve networks.\n    \n    Args:\n        public_bits: Number of bits in the public key.\n        private_bits: Number of bits in the private key.\n        dropout_rate: Dropout rate.\n    \n    Returns: \n        alice, bob, HO_model_addition, eve, abhemodel, m_train, p1_bits, evemodel, p2_bits, learning_rate, c3_bits, nonce_bits and HO_model_multiplication.\n    \"\"\"\n    learning_rate = 0.00005  # Adam and 0.0008\n\n    # Set up the crypto parameters: plaintext, key, and ciphertext bit lengths\n    # Plaintext 1 and 2\n    p1_bits = 16\n    p2_bits = 16\n\n    # nonce bits\n    nonce_bits = 64\n\n    # Ciphertext 1 and 2\n    c1_bits = (p1_bits+public_bits+nonce_bits)//2 \n    c2_bits = (p2_bits+public_bits+nonce_bits)//2 \n\n    c3_bits = (c1_bits+c2_bits)//2\n\n    pad = 'same'\n\n    # Size of the message space\n    m_train = 2**((p1_bits+p2_bits)//2) # mabye add p2_bits\n\n    # Define Alice inputs\n    ainput0 = Input(shape=(public_bits,))  # public key\n    ainput1 = Input(shape=(p1_bits))  # plaintext 1\n    ainput2 = Input(shape=(p2_bits))  # plaintext 2\n    anonce_input = Input(shape=(nonce_bits))  # nonce\n\n    aoutput_first = process_plaintext(ainput0, ainput1, anonce_input, p1_bits, public_bits, nonce_bits, dropout_rate, pad)\n    aoutput_second = process_plaintext(ainput0, ainput2, anonce_input, p2_bits, public_bits, nonce_bits, dropout_rate, pad)\n\n    alice = Model(inputs=[ainput0, ainput1, ainput2, anonce_input],\n                outputs=[aoutput_first, aoutput_second], name='alice')\n\n\n\n    # Generate the HO_model network with an input layer and two NAC layers for addition\n    units = 2\n    HOinput0_addition = Input(shape=(c3_bits)) # operation\n    HOinput1_addition = Input(shape=(c1_bits))  # ciphertext 1\n    HOinput2_addition = Input(shape=(c2_bits))  # ciphertext 2\n\n    HO_reshape0_addition = Reshape((c3_bits, 1))(HOinput0_addition)\n    HO_reshape1_addition = Reshape((c1_bits, 1))(HOinput1_addition)\n    HO_reshape2_addition = Reshape((c2_bits, 1))(HOinput2_addition)\n\n    HOinput_addition =  concatenate([HO_reshape0_addition, HO_reshape1_addition, HO_reshape2_addition], axis=-1)\n    x_a = NAC(units)(HOinput_addition)\n    x_a = NAC(1)(x_a)\n    x_a = Reshape((c3_bits,))(x_a)\n\n    HO_model_addition = Model(inputs=[HOinput0_addition, HOinput1_addition, HOinput2_addition], outputs=x_a)\n\n    # Generate the HO_model network with an input layer and two NALU layers for multiplication\n    units = 2\n    HOinput0_multiplication = Input(shape=(c3_bits)) # operation\n    HOinput1_multiplication = Input(shape=(c1_bits))  # ciphertext 1\n    HOinput2_multiplication = Input(shape=(c2_bits))  # ciphertext 2\n\n    HO_reshape0_multiplication = Reshape((c3_bits, 1))(HOinput0_multiplication)\n    HO_reshape1_multiplication = Reshape((c1_bits, 1))(HOinput1_multiplication)\n    HO_reshape2_multiplication = Reshape((c2_bits, 1))(HOinput2_multiplication)\n\n    HOinput_multiplication =  concatenate([HO_reshape0_multiplication, HO_reshape1_multiplication, HO_reshape2_multiplication], axis=-1)\n    x_m = NALU(units)(HOinput_multiplication)\n    x_m = NALU(1)(x_m)\n    x_m = Reshape((c3_bits,))(x_m)\n\n    HO_model_multiplication = Model(inputs=[HOinput0_multiplication, HOinput1_multiplication, HOinput2_multiplication], outputs=x_m)\n\n    # Bob network\n    binput0 = Input(shape=(c3_bits,))  # Input will be of shape c3\n    binput1 = Input(shape=(private_bits,))  # private key\n    bnonce_input = Input(shape=(nonce_bits))  # nonce\n\n    binput = concatenate([binput0, binput1, bnonce_input], axis=1)\n\n    bdense1 = Dense(units=((p1_bits+p2_bits)), activation='tanh')(binput)\n    breshape = Reshape(((p1_bits+p2_bits), 1,))(bdense1)\n\n    bconv1 = Conv1D(filters=2, kernel_size=4, strides=1,\n                    padding=pad, activation='tanh')(breshape)\n    bconv2 = Conv1D(filters=4, kernel_size=2, strides=2,\n                    padding=pad, activation='tanh')(bconv1)\n    bconv3 = Conv1D(filters=4, kernel_size=1, strides=1,\n                    padding=pad, activation='tanh')(bconv2)\n    bconv4 = Conv1D(filters=1, kernel_size=1, strides=1,\n                    padding=pad, activation='hard_sigmoid')(bconv3)\n\n    # Output corresponding to shape of p1 + p2\n    bflattened = Flatten()(bconv4)\n\n    boutput = Lambda(lambda x: x * 2)(bflattened)\n\n    bob = Model(inputs=[binput0, binput1, bnonce_input],\n                outputs=boutput, name='bob')\n\n\n    # Eve network\n    einput0 = Input(shape=(c3_bits,))  # Input will be of shape c3\n    einput1 = Input(shape=(public_bits, )) # public key\n    enonce_input = Input(shape=(nonce_bits))  # nonce\n\n\n    einput = concatenate([einput0, einput1, enonce_input], axis=1)\n\n    edense1 = Dense(units=((p1_bits+p2_bits)), activation='tanh')(einput)\n    edense2 = Dense(units=((p1_bits+p2_bits)), activation='tanh')(edense1)\n    ereshape = Reshape(((p1_bits+p2_bits), 1,))(edense2)\n\n    econv1 = Conv1D(filters=2, kernel_size=4, strides=1,\n                    padding=pad, activation='tanh')(ereshape)\n    econv2 = Conv1D(filters=4, kernel_size=2, strides=2,\n                    padding=pad, activation='tanh')(econv1)\n    econv3 = Conv1D(filters=4, kernel_size=1, strides=1,\n                    padding=pad, activation='tanh')(econv2)\n    econv4 = Conv1D(filters=1, kernel_size=1, strides=1,\n                    padding=pad, activation='hard_sigmoid')(econv3)\n\n    # Eve's attempt at guessing the plaintext, corresponding to shape of p1 + p2\n    eflattened = Flatten()(econv4)\n\n    eoutput = Lambda(lambda x: x * 2)(eflattened)\n\n\n    eve = Model([einput0, einput1, enonce_input], eoutput, name='eve')\n\n    # Loss and optimizer\n\n    # Alice gets two outputs from 3 inputs\n    aliceout1, aliceout2 = alice([ainput0, ainput1, ainput2, anonce_input])\n\n    # HO_model get one output from Alice's two output\n    HOout_addition = HO_model_addition([HOinput0_addition, aliceout1, aliceout2])\n    HOout_multiplication = HO_model_addition([HOinput0_multiplication, aliceout1, aliceout2])\n\n    # Eve and bob get one output from HO_model output with the size of p1+p2\n    bobout_addition = bob([HOout_addition, binput1, anonce_input]) \n    eveout_addition = eve([HOout_addition, ainput0, anonce_input])\n\n    # Eve and bob get one output from HO_model output with the size of p1*p2\n    bobout_multiplication = bob([HOout_multiplication, binput1, anonce_input]) \n    eveout_multiplication = eve([HOout_multiplication, ainput0, anonce_input])\n\n    # Eve and Bob output from alice to decrypt p1/p2\n    bobout_alice1 = bob([aliceout1, binput1, anonce_input])\n    eveout_alice1 = eve([aliceout1, ainput0, anonce_input])\n\n    bobout_alice2 = bob([aliceout2, binput1, anonce_input])\n    eveout_alice2 = eve([aliceout2, ainput0, anonce_input])\n\n    abhemodel = Model([ainput0, ainput1, ainput2, anonce_input, binput1, HOinput0_addition, HOinput0_multiplication],\n                    [bobout_addition, bobout_multiplication, bobout_alice1, bobout_alice2], name='abhemodel')\n\n    # Loss functions\n    eveloss_addition = K.mean(K.sum(K.abs(ainput1 + ainput2 - eveout_addition), axis=-1))\n    bobloss_addition = K.mean(K.sum(K.abs(ainput1 + ainput2 - bobout_addition), axis=-1))\n\n    eveloss_multiplication = K.mean(K.sum(K.abs(ainput1 * ainput2 - eveout_multiplication), axis=-1))\n    bobloss_multiplication = K.mean(K.sum(K.abs(ainput1 * ainput2 - bobout_multiplication), axis=-1))\n\n    eveloss_alice = K.mean(K.sum(K.abs(ainput1 - eveout_alice1), axis=-1))\n    bobloss_alice = K.mean(K.sum(K.abs(ainput1 - bobout_alice1), axis=-1))\n\n    eveloss_alice2 = K.mean(K.sum(K.abs(ainput2 - eveout_alice2), axis=-1))\n    bobloss_alice2 = K.mean(K.sum(K.abs(ainput2 - bobout_alice2), axis=-1))\n\n    eveloss = (eveloss_addition+eveloss_multiplication+eveloss_alice+eveloss_alice2)/4\n    bobloss = (bobloss_addition+bobloss_multiplication+bobloss_alice+bobloss_alice2)/4\n\n    # Build and compile the ABHE model, used for training Alice, Bob and HE networks\n    abheloss = bobloss + K.square((p1_bits+p2_bits)/2 - eveloss) / ((p1_bits+p2_bits//2)**2)\n    abhemodel.add_loss(abheloss)\n\n    # Set the Adam optimizer\n    beoptim = Adam(learning_rate=learning_rate)\n    eveoptim = Adam(learning_rate=learning_rate)\n    optimizer_a = RMSprop(0.1)\n    HO_model_addition.compile(optimizer_a, 'mse')\n    optimizer_m = RMSprop(0.1)\n    HO_model_multiplication.compile(optimizer_m, 'mse')\n    abhemodel.compile(optimizer=beoptim)\n\n    # Build and compile the Eve model, used for training Eve net (with Alice frozen)\n    alice.trainable = False\n    evemodel = Model([ainput0, ainput1, ainput2, anonce_input, HOinput0_addition, HOinput0_multiplication], [eveout_addition, eveout_multiplication, eveout_alice1, eveout_alice2], name='evemodel')\n    evemodel.add_loss(eveloss)\n    evemodel.compile(optimizer=eveoptim)\n\n    return alice, bob, HO_model_addition, eve, abhemodel, m_train, p1_bits, evemodel, p2_bits, learning_rate, c3_bits, nonce_bits, HO_model_multiplication\n\nif __name__ == \"__main__\":\n    # Public and private key, changed to fit the key generated in EllipticCurve.py\n    curve = set_curve(\"secp256r1\")\n    public_bits = get_key_shape(curve)[1]  \n    private_bits = get_key_shape(curve)[0]\n    dropout_rate = 0.6\n    alice, bob, HO_model_addition, eve, abhemodel, m_train, p1_bits, evemodel, p2_bits, learning_rate, c3_bits, nonce_bits, HO_model_multiplication = create_networks(public_bits, private_bits, dropout_rate)",
      "info": {
        "size": 11422,
        "last_modified": "2025-01-17T14:10:01.531978",
        "mime_type": "text/x-python",
        "extension": ".py"
      }
    }
  ]
}